{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Layout.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaylaque/glauco-scan/blob/main/Layout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbgYFni1ZPwd",
        "outputId": "3d3e5919-20bd-4894-93fd-8d6efa8e80e4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4SNMvjAbtl8Q",
        "outputId": "e168633e-b4a9-47ba-9059-f03bd615e762"
      },
      "source": [
        "!pip install streamlit\n",
        "!pip install pyqrcode\n",
        "!pip install pypng\n",
        "!pip uninstall keras -y\n",
        "!pip uninstall keras-nightly -y\n",
        "!pip uninstall keras-Preprocessing -y\n",
        "!pip uninstall keras-vis -y\n",
        "!pip uninstall tensorflow -y\n",
        "!pip install tensorflow==2.3.0\n",
        "!pip install keras==2.4\n",
        "!pip install git+https://github.com/qubvel/segmentation_models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-0.85.1-py2.py3-none-any.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.2.0)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.0)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (5.1.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.0.0)\n",
            "Collecting gitpython\n",
            "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 61.6 MB/s \n",
            "\u001b[?25hCollecting watchdog\n",
            "  Downloading watchdog-2.1.3-py3-none-manylinux2014_x86_64.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.4 MB/s \n",
            "\u001b[?25hCollecting validators\n",
            "  Downloading validators-0.18.2-py3-none-any.whl (19 kB)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.6.2-py2.py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 50.3 MB/s \n",
            "\u001b[?25hCollecting base58\n",
            "  Downloading base58-2.1.0-py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.1.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.17.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: click<8.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Collecting blinker\n",
            "  Downloading blinker-1.4.tar.gz (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 69.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.3)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.11.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit) (2018.9)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit) (1.15.0)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.0.5)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.6.3)\n",
            "Collecting ipykernel>=5.1.2\n",
            "  Downloading ipykernel-6.0.3-py3-none-any.whl (122 kB)\n",
            "\u001b[K     |████████████████████████████████| 122 kB 69.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter-client<7.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (5.3.5)\n",
            "Collecting importlib-metadata<4\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Collecting ipython<8.0,>=7.23.1\n",
            "  Downloading ipython-7.25.0-py3-none-any.whl (786 kB)\n",
            "\u001b[K     |████████████████████████████████| 786 kB 42.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
            "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (3.7.4.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.18.0)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.19-py3-none-any.whl (368 kB)\n",
            "\u001b[K     |████████████████████████████████| 368 kB 52.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (2.6.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (57.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.4.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.1.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.0.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<7.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.7.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<7.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (22.1.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.7.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.10.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.3 MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.4.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2.10)\n",
            "Building wheels for collected packages: blinker\n",
            "  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for blinker: filename=blinker-1.4-py3-none-any.whl size=13479 sha256=393d6c8f61b81ba6ae25895b786a2287fee68d3da2948378dd9165482d3b3b8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/f5/18/df711b66eb25b21325c132757d4314db9ac5e8dabeaf196eab\n",
            "Successfully built blinker\n",
            "Installing collected packages: prompt-toolkit, ipython, importlib-metadata, ipykernel, smmap, gitdb, watchdog, validators, pydeck, gitpython, blinker, base58, streamlit\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.6.1\n",
            "    Uninstalling importlib-metadata-4.6.1:\n",
            "      Successfully uninstalled importlib-metadata-4.6.1\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 4.10.1\n",
            "    Uninstalling ipykernel-4.10.1:\n",
            "      Successfully uninstalled ipykernel-4.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.19 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.0.3 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.25.0 which is incompatible.\u001b[0m\n",
            "Successfully installed base58-2.1.0 blinker-1.4 gitdb-4.0.7 gitpython-3.1.18 importlib-metadata-3.10.1 ipykernel-6.0.3 ipython-7.25.0 prompt-toolkit-3.0.19 pydeck-0.6.2 smmap-4.0.0 streamlit-0.85.1 validators-0.18.2 watchdog-2.1.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "ipykernel",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting pyqrcode\n",
            "  Downloading PyQRCode-1.2.1.zip (41 kB)\n",
            "\u001b[?25l\r\u001b[K     |███████▉                        | 10 kB 33.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 20 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 30 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 40 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 41 kB 510 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyqrcode\n",
            "  Building wheel for pyqrcode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyqrcode: filename=PyQRCode-1.2.1-py3-none-any.whl size=36246 sha256=d9bfba0b1e56e7b77bc26920758beecb9f248ace2f4197b74689dbc10f11289e\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/14/ad/f09824edf35fdc5fd8acc01b60ff57f858f212a2d5c41dddd2\n",
            "Successfully built pyqrcode\n",
            "Installing collected packages: pyqrcode\n",
            "Successfully installed pyqrcode-1.2.1\n",
            "Collecting pypng\n",
            "  Downloading pypng-0.0.20.tar.gz (649 kB)\n",
            "\u001b[K     |████████████████████████████████| 649 kB 8.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pypng\n",
            "  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypng: filename=pypng-0.0.20-py3-none-any.whl size=67177 sha256=3ad26a24f87d034029cbfad7fabaa4ecbd0a37078f634f04d5ea5190aa3f9cb1\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/64/43/dfd10cf95dc1687dc5350e861321ecd9a5d76b7c3d96fa1dc6\n",
            "Successfully built pypng\n",
            "Installing collected packages: pypng\n",
            "Successfully installed pypng-0.0.20\n",
            "Found existing installation: Keras 2.4.3\n",
            "Uninstalling Keras-2.4.3:\n",
            "  Successfully uninstalled Keras-2.4.3\n",
            "Found existing installation: keras-nightly 2.5.0.dev2021032900\n",
            "Uninstalling keras-nightly-2.5.0.dev2021032900:\n",
            "  Successfully uninstalled keras-nightly-2.5.0.dev2021032900\n",
            "Found existing installation: Keras-Preprocessing 1.1.2\n",
            "Uninstalling Keras-Preprocessing-1.1.2:\n",
            "  Successfully uninstalled Keras-Preprocessing-1.1.2\n",
            "Found existing installation: keras-vis 0.4.1\n",
            "Uninstalling keras-vis-0.4.1:\n",
            "  Successfully uninstalled keras-vis-0.4.1\n",
            "Found existing installation: tensorflow 2.5.0\n",
            "Uninstalling tensorflow-2.5.0:\n",
            "  Successfully uninstalled tensorflow-2.5.0\n",
            "Collecting tensorflow==2.3.0\n",
            "  Downloading tensorflow-2.3.0-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4 MB 45 kB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (3.17.3)\n",
            "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
            "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
            "\u001b[K     |████████████████████████████████| 459 kB 61.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.36.2)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.2.0)\n",
            "Collecting keras-preprocessing<1.2,>=1.1.1\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (2.5.0)\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 51.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.4.1)\n",
            "Collecting numpy<1.19.0,>=1.16.0\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.34.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (3.3.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.12.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.32.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (57.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.10.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.5.0)\n",
            "Installing collected packages: numpy, tensorflow-estimator, keras-preprocessing, h5py, gast, tensorflow\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.5.0\n",
            "    Uninstalling tensorflow-estimator-2.5.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.0.3 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.25.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.3.3 h5py-2.10.0 keras-preprocessing-1.1.2 numpy-1.18.5 tensorflow-2.3.0 tensorflow-estimator-2.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.4\n",
            "  Downloading Keras-2.4.0-py2.py3-none-any.whl (170 kB)\n",
            "\u001b[?25l\r\u001b[K     |██                              | 10 kB 35.6 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 20 kB 36.0 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 30 kB 22.5 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 40 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 51 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 61 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 71 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 81 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 92 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 102 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 112 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 122 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 133 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 143 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 153 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 163 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 170 kB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.4) (2.3.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.4) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.4) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.4) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.4) (1.4.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (3.17.3)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (2.5.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (1.34.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (0.12.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (0.3.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (0.36.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (1.1.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (2.3.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (57.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (0.6.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (1.32.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (3.3.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (3.10.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (3.5.0)\n",
            "Installing collected packages: keras\n",
            "Successfully installed keras-2.4.0\n",
            "Collecting git+https://github.com/qubvel/segmentation_models\n",
            "  Cloning https://github.com/qubvel/segmentation_models to /tmp/pip-req-build-c1tfha_n\n",
            "  Running command git clone -q https://github.com/qubvel/segmentation_models /tmp/pip-req-build-c1tfha_n\n",
            "  Running command git submodule update --init --recursive -q\n",
            "Collecting keras_applications<=1.0.8,>=1.0.7\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting image-classifiers==1.0.0\n",
            "  Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting efficientnet==1.0.0\n",
            "  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from efficientnet==1.0.0->segmentation-models==1.0.1) (0.16.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras_applications<=1.0.8,>=1.0.7->segmentation-models==1.0.1) (1.18.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras_applications<=1.0.8,>=1.0.7->segmentation-models==1.0.1) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras_applications<=1.0.8,>=1.0.7->segmentation-models==1.0.1) (1.15.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (2.4.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (7.1.2)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (3.2.2)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (1.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (2.5.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (0.10.0)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (4.4.2)\n",
            "Building wheels for collected packages: segmentation-models\n",
            "  Building wheel for segmentation-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segmentation-models: filename=segmentation_models-1.0.1-py3-none-any.whl size=33809 sha256=b415442710287432cd19da0f0202e5b0bf8618730bc7125ec8044e2ab89a92d2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3bbl8u5w/wheels/02/cd/18/61c0bbb8766acfec68f9d20618886b7b38dfeeb95865b6ba00\n",
            "Successfully built segmentation-models\n",
            "Installing collected packages: keras-applications, image-classifiers, efficientnet, segmentation-models\n",
            "Successfully installed efficientnet-1.0.0 image-classifiers-1.0.0 keras-applications-1.0.8 segmentation-models-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Yx-J9ecF62w",
        "outputId": "9a12e13a-30c4-4d92-bcbe-3351b44b09c6"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-30 01:50:39--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 34.192.220.41, 34.196.64.221, 44.196.181.185, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|34.192.220.41|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  17.9MB/s    in 0.7s    \n",
            "\n",
            "2021-07-30 01:50:40 (17.9 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13832437/13832437]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhldFxZ7GKW_",
        "outputId": "916b7842-b7fe-4fd5-e110-78c002fb2ef2"
      },
      "source": [
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhy05EzxGM9m"
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 8501 &')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKNsvZW_GP6k",
        "outputId": "81676887-6aec-4c63-e6fe-4548fc8d27a7"
      },
      "source": [
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    'import sys, json; print(\"Execute the next cell and the go to the following URL: \" +json.load(sys.stdin)[\"tunnels\"][0][\"public_url\"])'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Execute the next cell and the go to the following URL: https://faafc4a14b11.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4nyK1DnZXWA"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SI4LuWYXtsKj",
        "outputId": "1351c28b-ead2-422c-a25c-dd126290566a"
      },
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image, ImageOps\n",
        "import pyqrcode\n",
        "import sys\n",
        "import png\n",
        "import matplotlib.pyplot as plt\n",
        "import segmentation_models as sm\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import np_utils\n",
        "import tensorflow as tf\n",
        "import skimage.exposure\n",
        "import glob\n",
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "from pylab import*\n",
        "from scipy import signal\n",
        "from matplotlib import pyplot as plt\n",
        "from skimage.feature import greycomatrix, greycoprops\n",
        "from skimage import data\n",
        "import joblib\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, \\\n",
        "    Conv2D, MaxPooling2D, ZeroPadding2D, Input, Embedding, \\\n",
        "    Lambda, UpSampling2D, Cropping2D, Concatenate\n",
        "\n",
        "\n",
        "title='<p style=\"font-family:Courier; color: Blue; font size: 35px; text_align: center; font_weight: bold;\">Glaucoma OCT Scan Detector Webapp Using Transfer Learning</p>'\n",
        "st.markdown(title,unsafe_allow_html=True)\n",
        "\n",
        "sub_title='<p style=\"font-family:Arial; color: White; font size: 12px; text_align: center;\">by gumigummy</p>'\n",
        "st.markdown(sub_title,unsafe_allow_html=True)\n",
        "\n",
        "github='<p style=\"font-family:Arial; color: Green; font size: 13px; text_align: center;\">[Github]</p>'\n",
        "st.markdown(github,unsafe_allow_html=True)\n",
        "\n",
        "st.info(\"Webapp ini akan mendeteksi ada tidaknya glaucoma dari foto fundus mata\")\n",
        "\n",
        "instruct='<p style=\"font-family:Courier; color: Light Blue; font size: 15px;\">Silakan upload foto fundus mata di bawah ini.</p>'\n",
        "st.markdown(instruct,unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "col1,col2 = st.beta_columns(2)\n",
        "image = col1.file_uploader(\"Foto fundus\",type=['png','jpg','jpeg'])\n",
        "\n",
        "col1,col2 = st.beta_columns(2)\n",
        "predict_button = col1.button('Predict on uploaded files')\n",
        "\n",
        "if image is not None:\n",
        "    st.success('File upload successful')\n",
        "    original = Image.open(image)\n",
        "    st.image(original, use_column_width=True)\n",
        "    K.set_image_data_format('channels_first')\n",
        "    \n",
        "    def preprocess(image):\n",
        "      image = image / 255.0 #batch_X\n",
        "      image = th_to_tf_encoding(image)\n",
        "      image = [skimage.exposure.equalize_adapthist(image[i]) \n",
        "      for i in range(len(image))]\n",
        "      image = np.array(image)\n",
        "      image = tf_to_th_encoding(image)\n",
        "      return image\n",
        "\n",
        "    def tf_to_th_encoding(X):\n",
        "      return np.rollaxis(X, 3, 1)\n",
        "    def th_to_tf_encoding(X):\n",
        "      return np.rollaxis(X, 1, 4)\n",
        "\n",
        "    def model_to_use(model_path, size):\n",
        "      BACKBONE = 'efficientnetb7'\n",
        "      model_freeze = sm.Unet(BACKBONE, input_shape = (3, None, None), classes=1, encoder_weights='imagenet', encoder_freeze = True)\n",
        "      model_freeze.compile(SGD(lr=1e-4, momentum=0.95),\n",
        "                           loss=sm.losses.DiceLoss(),\n",
        "                           metrics=[sm.metrics.iou_score, sm.metrics.FScore()]\n",
        "                           )\n",
        "      model_freeze.load_weights(model_path)\n",
        "      imageI = Image.open(image)\n",
        "      imageI = ImageOps.fit(imageI, (size,size),Image.ANTIALIAS)\n",
        "      imageI = imageI.convert('RGB')\n",
        "      img = np.asarray(imageI)\n",
        "      img = np.expand_dims(img, axis = 0)\n",
        "      img = tf_to_th_encoding(img)\n",
        "      img = preprocess(img)\n",
        "      pred = model_freeze.predict(img)\n",
        "      return pred\n",
        "    \n",
        "    model_path_disc = '/content/drive/MyDrive/HackathonIGI/optic-nerve-cnn-1.0.0/models_weights/29.07,02:36,OD Cup, U-Net efficientnetb7 freeze DRISHTI-GS 512 px cropped to OD 128 px fold 0, Adam, Dice Loss/last_checkpoint.hdf5'\n",
        "    model_path_cup = '/content/drive/MyDrive/HackathonIGI/optic-nerve-cnn-1.0.0/models_weights/29.07,16:13,OD Cup, U-Net efficientnetb7 freeze RIM ONE 512 px cropped to OD 128 px fold 0, SDG, Dice Loss/last_checkpoint.hdf5'\n",
        "\n",
        "    pred_disc = model_to_use(model_path_disc, 256)\n",
        "    pred_cup = model_to_use(model_path_cup, 128)\n",
        "    \n",
        "    imageI = Image.open(image)\n",
        "    # imageI = ImageOps.fit(imageI, (size,size),Image.ANTIALIAS)\n",
        "    imageI = imageI.convert('RGB')\n",
        "    img = np.asarray(imageI)\n",
        "    fig = plt.figure(figsize=(9, 4))\n",
        "    ax = fig.add_subplot(1, 3, 1)\n",
        "    pred = pred_disc[0].reshape(256,256)\n",
        "    print(pred.shape)\n",
        "    ax.imshow(pred, cmap=plt.cm.Greys_r)\n",
        "    ax.set_title('Disc')\n",
        "    ax = fig.add_subplot(1, 3, 2)\n",
        "    predu = pred_cup[0].reshape(128,128)\n",
        "    print(predu.shape)\n",
        "    print(type(img))\n",
        "    ax.imshow(predu, cmap=plt.cm.Greys_r)\n",
        "    ax.set_title('Cup')\n",
        "    plt.show()\n",
        "\n",
        "    plt.imsave('cup.png', predu, cmap=plt.cm.Greys_r)\n",
        "    plt.imsave('disc.png', pred, cmap=plt.cm.Greys_r)\n",
        "    cupopen = Image.open('cup.png')\n",
        "    discopen = Image.open('disc.png')\n",
        "    st.text('Cup')\n",
        "    st.image(cupopen, use_column_width=True)\n",
        "    st.text ('Disc')\n",
        "    st.image(discopen, use_column_width=True)\n",
        "\n",
        "    # CDR Calculation\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(10,10))\n",
        "    def cdr(cup,disc,plot):\n",
        "      #morphological closing and opening operations\n",
        "      R1 = cv2.morphologyEx(cup, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(2,2)), iterations = 1)\n",
        "      r1 = cv2.morphologyEx(R1, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(7,7)), iterations = 1)\n",
        "      R2 = cv2.morphologyEx(r1, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(1,21)), iterations = 1)\n",
        "      r2 = cv2.morphologyEx(R2, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(21,1)), iterations = 1)\n",
        "      R3 = cv2.morphologyEx(r2, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(33,33)), iterations = 1)\t\n",
        "      r3 = cv2.morphologyEx(R3, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(43,43)), iterations = 1)\n",
        "      \n",
        "      img = clahe.apply(r3)\n",
        "      \n",
        "      ret,thresh = cv2.threshold(cup,127,255,0)\n",
        "      # print(thresh)\n",
        "      # print(len(cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)))\n",
        "      contours,hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) #Getting all possible contours in the segmented image\n",
        "      cup_diameter = 0\n",
        "      largest_area = 0\n",
        "      el_cup = contours[0]\n",
        "      if len(contours) != 0:\n",
        "        for i in range(len(contours)):\n",
        "          if len(contours[i]) >= 5:\n",
        "            area = cv2.contourArea(contours[i]) #Getting the contour with the largest area\n",
        "            if (area>largest_area):\n",
        "              largest_area=area\n",
        "              index = i\n",
        "              el_cup = cv2.fitEllipse(contours[i])\n",
        "              \n",
        "      cv2.ellipse(img,el_cup,(140,60,150),3)  #fitting ellipse with the largest area\n",
        "      x,y,w,h = cv2.boundingRect(contours[index]) #fitting a rectangle on the ellipse to get the length of major axis\n",
        "      cup_diameter = max(w,h) #major axis is the diameter\n",
        "      \n",
        "      #morphological closing and opening operations\n",
        "      R1 = cv2.morphologyEx(disc, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(2,2)), iterations = 1)\n",
        "      r1 = cv2.morphologyEx(R1, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(7,7)), iterations = 1)\n",
        "      R2 = cv2.morphologyEx(r1, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(1,21)), iterations = 1)\n",
        "      r2 = cv2.morphologyEx(R2, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(21,1)), iterations = 1)\n",
        "      R3 = cv2.morphologyEx(r2, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(33,33)), iterations = 1)\n",
        "      r3 = cv2.morphologyEx(R3, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(43,43)), iterations = 1)\n",
        "      \n",
        "      img2 = clahe.apply(r3)\n",
        "      \n",
        "      ret,thresh = cv2.threshold(disc,127,255,0)\n",
        "      contours,hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE) #Getting all possible contours in the segmented image\n",
        "      disk_diameter = 0\n",
        "      largest_area = 0\n",
        "      el_disc = el_cup\n",
        "      if len(contours) != 0:\n",
        "        for i in range(len(contours)):\n",
        "          if len(contours[i]) >= 5:\n",
        "            area = cv2.contourArea(contours[i]) #Getting the contour with the largest area\n",
        "            if (area>largest_area):\n",
        "              largest_area=area\n",
        "              index = i\n",
        "              el_disc = cv2.fitEllipse(contours[i])\n",
        "      cv2.ellipse(img2,el_disc,(140,60,150),3) #fitting ellipse with the largest area\n",
        "      x,y,w,h = cv2.boundingRect(contours[index]) #fitting a rectangle on the ellipse to get the length of major axis\n",
        "      disk_diameter = max(w,h) #major axis is the diameter\n",
        "      \n",
        "      if plot:\n",
        "        plt.imshow(img2, cmap = 'gray', interpolation = 'bicubic')\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(\"Optic Disk\")\n",
        "        plt.show()\n",
        "        plt.imshow(img)\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(\"Optic Cup\")\n",
        "        plt.show()\n",
        "\n",
        "      if(disk_diameter == 0): return 1 # if disc not segmented properly then cdr might be infinity\n",
        "      cdr = cup_diameter/disk_diameter #ration of major axis of cup and disc\n",
        "      return cdr\n",
        "    \n",
        "    # use this if the input from a directory\n",
        "    cup = cv2.imread('./cup.png',0) \n",
        "    disc = cv2.imread('./disc.png',0) \n",
        "\n",
        "    cdr_cal = cdr(cup,disc,True)\n",
        "    print(cdr_cal)\n",
        "    cdrqr = pyqrcode.creater(cdr_cal)\n",
        "    cdrqr.png('qr.png')\n",
        "    qrcode = Image.open('qr.png')\n",
        "    st.image(qrcode, use_column_width=True)\n",
        "\n",
        "    \n",
        "    model_name_classify = 'svm_kf10_83.6% acc.sav'\n",
        "    filepath = '/content/drive/MyDrive/HackathonIGI/' + model_name_classify\n",
        "    def predict(CDR, model_file):\n",
        "      loaded_model = joblib.load(filepath)\n",
        "      Xtest = pd.DataFrame([CDR])\n",
        "      result = loaded_model.predict(Xtest)\n",
        "      return result\n",
        "    pred = predict(cdr_cal, filepath)\n",
        "    if pred[0] == 0:\n",
        "      print('Normal')\n",
        "      st.text('Normal')\n",
        "    else:\n",
        "      print('Glaukoma')\n",
        "      st.text('Glaukoma')\n",
        "    \n",
        "\n",
        "#imageqr = pyqrcode.create(uploaded_image)\n",
        "# imageqr.png('QR.png')\n",
        "# imgqr = cv2.imread('QR.png')\n",
        "# show = cv2.imshow('QR.png', imgqr)\n",
        "# st.image(imgqr, use_column_width=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqBu9bvuuSZq",
        "outputId": "e011e1df-f54a-4583-a7b6-f0bfb7cda982"
      },
      "source": [
        "!streamlit run app.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-30 02:09:07.292081: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.2:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.233.185.108:8501\u001b[0m\n",
            "\u001b[0m\n",
            "Using TensorFlow backend.\n",
            "Segmentation Models: using `keras` framework.\n",
            "2021-07-30 02:12:05.037577: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2021-07-30 02:12:05.070912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-30 02:12:05.071517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-07-30 02:12:05.071579: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-30 02:12:05.073669: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-30 02:12:05.075510: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-30 02:12:05.075931: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-30 02:12:05.077974: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-30 02:12:05.079239: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-30 02:12:05.083234: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-30 02:12:05.083426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-30 02:12:05.084109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-30 02:12:05.084662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2021-07-30 02:12:05.085206: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-07-30 02:12:05.090709: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2199995000 Hz\n",
            "2021-07-30 02:12:05.090889: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56369ba07d40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-07-30 02:12:05.090916: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2021-07-30 02:12:05.296919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-30 02:12:05.297729: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5636a1cb8540 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2021-07-30 02:12:05.297759: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2021-07-30 02:12:05.297959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-30 02:12:05.298514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-07-30 02:12:05.298574: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-30 02:12:05.298619: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-30 02:12:05.298646: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-30 02:12:05.298670: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-30 02:12:05.298697: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-30 02:12:05.298722: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-30 02:12:05.298746: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-30 02:12:05.298828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-30 02:12:05.299413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-30 02:12:05.299906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2021-07-30 02:12:05.299971: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-30 02:12:05.821203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-30 02:12:05.821268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2021-07-30 02:12:05.821284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2021-07-30 02:12:05.821529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-30 02:12:05.822189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-30 02:12:05.822755: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-07-30 02:12:05.822804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13970 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2021-07-30 02:12:18.935931: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-30 02:12:20.154832: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "(256, 256)\n",
            "(128, 128)\n",
            "<class 'numpy.ndarray'>\n",
            "(256, 256)\n",
            "(128, 128)\n",
            "<class 'numpy.ndarray'>\n",
            "3.4285714285714284\n",
            "Glaukoma\n",
            "2021-07-30 02:14:18.344 Got an empty FileUploader widget_value. (We expect a list with at least one value in it.)\n",
            "2021-07-30 02:14:43.055 Got an empty FileUploader widget_value. (We expect a list with at least one value in it.)\n",
            "2021-07-30 02:14:55.548 5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8d41d1c320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2021-07-30 02:15:09.065 6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8d4e025f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "(256, 256)\n",
            "(128, 128)\n",
            "<class 'numpy.ndarray'>\n",
            "2021-07-30 02:15:23.830 7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8df95b7d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2021-07-30 02:15:36.361 8 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8d4e7934d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "(256, 256)\n",
            "(128, 128)\n",
            "<class 'numpy.ndarray'>\n",
            "3.4285714285714284\n",
            "Glaukoma\n",
            "2021-07-30 02:19:03.237 9 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8d4e6b0dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2021-07-30 02:19:16.474 10 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8d56058830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "(256, 256)\n",
            "(128, 128)\n",
            "<class 'numpy.ndarray'>\n",
            "3.4285714285714284\n",
            "Glaukoma\n",
            "2021-07-30 02:21:35.967 Got an empty FileUploader widget_value. (We expect a list with at least one value in it.)\n",
            "2021-07-30 02:21:48.405 11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8d1e6439e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2021-07-30 02:22:01.163 11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8d1e281dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "(256, 256)\n",
            "(128, 128)\n",
            "<class 'numpy.ndarray'>\n",
            "2021-07-30 02:22:16.122 11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8d1f109dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2021-07-30 02:22:29.805 11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8d4f37d9e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "(256, 256)\n",
            "(128, 128)\n",
            "<class 'numpy.ndarray'>\n",
            "3.4285714285714284\n",
            "Glaukoma\n",
            "2021-07-30 02:49:21.449 11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8d4fa9a290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2021-07-30 02:49:34.391 11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8d4418fb90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "(256, 256)\n",
            "(128, 128)\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}